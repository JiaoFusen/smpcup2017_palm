caffe study ( 5 ) - AlexNet 之 算法 篇
在 机器学习 中 ， 我们 通常 要 考虑 的 一个 问题 是 如何 的 “ 以偏概全 ” ， 也 就是 以 有限 的 样本 或者 结构 去 尽可能 的 近 全局 的 分布 。 这 就要 在 样本 以及 结构 模型 上下 一些 工夫 。 在 一般 的 训练任务 中 ， 考虑 的 关键问题 之一 就是 数据分布 是否 合理 ： 首先 是 数据 集 的 覆盖度 ， 也 就是 数据 集 是否 能够 覆盖 样本空间 ； 其次 还要 尽可能 的 保证 具有 和 真实 数据 一样 的 分布 （ 注意 数据分布 是 未知 的 ， 你 只能 根据 一些 先验 来近 ） ， 这样 的 数据 才 是 有效 的 。 当然 这些 方式 只是 增大 了 得到 正确 解 的 概率 ， 而 并 不能 保证 一定 可以 得到 正确 解 。 当 你 不 知道 你 所取 的 训练 集合 是否 和 真实 分布 一致 的 时候 ， 那么 就要 多取 几次 ， 每 一个 数据 集都 算算 ， 对于 分类器 也 是 这样 ， 单个 分类器 往往 不能 精确 描述 一个 分界 面 ， 那么 我们 就 组合 一下 ， 每个 都 算算 。 从 方法论 上 讲 ， 对于 事物 观察 到 的 往往 是 局部 ， 因此 会犯 以偏概全 的 错误 ， 如果 能够 将 所 得到 的 “ 偏 ” ensambling 一下 ， 那么 就 生成 了 相对 的 “ 全 ” ， 从而 可以 更大 的 概率 近 总体 分布 。 这种 思想 在 好多 方面 都 体现 出来 ， 如 交叉 验证 ， 经典 的 ransac ， random tree （ forest ） ， adaboost 等 方法 。 下面 将 从 数据 和 模型 两个 方面 来 学习 一下 alexnet 中 的 一些 技巧 ， 主要 参考 的 是 alex 2012 年 的 nips 论文 imagenet classification with deep convolutional neural networks . 1 . 数据 的 处理 ： 到 目前为止 ， 还 没有 人 看到 数据 集 的 大小 对 deeplearning 算 法理 论 上限 造成 的 影响 ， 也就是说 数据 集合 还 没有 达到 临界点 ， 所以 增加 数据 集 只有 好处 ， 没有 坏处 。 在 alex 的 论 文中 ， 采用 了 两个 方 法 对于 图 像 进行 了 增强 。 a . 增大 训练样本 ： 通过 对于 图像 的 变换 实现 了 对于 数据 集合 的 enlarge 。 首先 对于 输入 的 图 像 （ size 256 * 256 ） 随机 提取 224 * 224 的 图 像 集合 ， 并 对 他们 做 一个 horizontal reflections 。 变换 后图 像 和 原图 像 相差 了 32 个 像素 ， 因此 主体 部分 应该 都 包含 在 训练 集合 中 ， 相当于 在 位置 这个 维度 上 丰富 了 训练 数据 。 对 horizontal reflections 来说 ， 相当于 相机 在 主轴 方向 做 了 镜像 ， 丰富 了 反 方向 的 图 像 。 数据 集合 增大 了 2048 倍 ， 直接 结果 就是 降低 了 overfitting 同时 降低 了 网络结构 设计 的 复杂 层度 。 在 测试阶段 ， 取 每 一个 测 试样 本 四个 角 以及 中间 区域 ， 一共 5 个 patch 然后 再 镜像 后 得到 10 个 样本 输入 到 网络 中 ， 最后 将 10 个 softmax 输出 平均 后 作为 最后 的 输出 。 b . 使用 pca 对于 训练 数据 进行 增强 ： 对于 每 一个 rgb 图像 进行 一个 pca 的 变换 ， 完成 去 噪 功能 ， 同时 为了 保证 图像 的 多样性 ， 在 eigenvalue 上加 了 一个 随机 的 尺度 因子 ， 每 一轮 重新 生成 一个 尺度 因子 ， 这样 保证 了 同一 副 图 像 中 在 显著 特征 上 有 一定 范围 的 变换 ， 降低 了 overfitting 的 概率 。 以上 的 策略 是不是 真的 有 必要 ， 这个 还是 要 打 一个 问号 ， 因为 对于 a 部分 来说 ， 样本 少 ， 可以 在 结构设计 上下 下功夫 ， 可能 达到 相同 的 效果 。 对于 b 来说 ， deeplearning 还 需要 对于 图 像 加入 增强 处理 吗 ？ 如果 这样的话 ， 自然 也 可以 用 一些 传统 人工 特征 先 来 一遍 ， 再 deeplearning 了 。 我 想 关键 的 原因 是 deeplearning 还 没有 真正 的 被 证明 的 规则 ， 所以 你 用 什么 策略 都 有点 道理 ， 但是 谁 敢 保证 不是 “ 以偏概全 ” 呢 ？ 2 . 模型 结构 ： 在 模型 的 设计 上 ， alexnet 做 了 一个 local response normalization 的 处理 ， 同时 在 节点 的 选择 上 采用 了 一个 dropout 策略 。 a . local response normalization . 公式 如下 ， 其中 a 是 每 一个 神经元 的 激活 ， n 是 在 同一个 位置 上 临近 的 kernel map 的 数目 ， n 是 可 kernel 的 总 数目 ， k ， alpha ， beta 都 是 预设 的 一些 hyper - parameters ， 其中 k = 2 ， n = 5 ， alpha = 1 * e - 4 ， beta = 0.75 。 从 这个 公式 中 可以 看出 ， 原来 的 激活 a 被加 一个 归一化 权重 （ 分母 部分 ） 生成 了 新 的 激活 b ， 相当于 在 同一个 位置 （ x ， y ） ， 不同 的 map 上 的 激活 进行 了 平滑 ， 但是 至于 为什么 k ， alpha ， beta 这样 来 设置 ， 没有 想 太 清楚 。 这个 平滑 大概 可以 将 识别率 提高 1 - 2 个 百分点 。 b . dropout 策略 使用 多个 model 来 共同 进行 预测 是 一个 降低 test errors 的 基本 方 法 ， 但是 单独 的 训练 多个 model 组合 会 导致 整个 的 训练 成本增加 ， 毕竟 训练 一个 单一 的 网络 需要 很长 的 时间 ， 即便 计算资源 足够 ， 在 不 影响 精度 的 情况 下 降低 整个 运算 时间 还是 我们 追求 的 目标 。 由此 hinton 提出 了 dropout 策略 ， 这个 策略 很 简单 ， 对于 每 一个 隐层 的 output ， 以 50 % 的 概率 将 他们 设置 为 0 ， 不再 对于 forward 或者 backward 的 过程 起 任何 作用 。 对于 每 一个 input 来说 ， 使用 的 不同 的 网络结构 ， 但是 权重 是 共享 的 。 这样 求得 的 参数 能够 适应 不同 的 情况 下 的 网络结构 ， 也 就是 提高 了 系统 的 泛化 能 力 。 在 alexnet 中 最后 的 两个 full - connected 层中 使用 了 这个 策略 。 3 . 优化 算法 的 参数 论 文中 使用 sgd 算法 ， 基本 参数设置 在 前面 优化 算法 的 总结 中 已经 提到 了 。 这里 要说 几个 个人 体会 。 a . 原文中 输入 的 batch 数目 是 256 ， 应该 alex 经过 调节 后 的 结果 ， 我 实际 用到 的 机器 性能比 较 低 ， 内存 8g ， 显存 4g ， 所以 不得不 就 将 batch 数目 往 下调 到 64 ， 以免 产生 out of memory 的 错误 。 这样 就 需要 调节 其他 的 参数 来 保证数据 的 收敛 。 原因 是 batch 比较 小 ， 导致 本文 开篇 提到 的 样本 覆盖面 过低 ， 产生 了 非常 多 的 局部 极 小点 ， 在 步长 和 方向 的 共同 作用 下 ， 导致 数据 产生 了 震荡 ， 导致 了 不 收敛 。 b . 在 这种 情况 下 ， 把 learning rate 调节 到 了 0.02 ， 相当于 加大 了 步长 ， 这样 可以 在 一定 程度 上 避免 震荡 ， 可以 越过 局部 极 小点 往比 较大 的 极点 行走 。 c . 对于 每 一层 的 bias 从 1 设置 为了 0.1 ， 在 一定 程度 上 限制 了 激活 的 大小 ， 这样 就 限制 了 某 一过 大 的 误差 的 影响 ， 这样 可以 避免 迭代 方向 出现 过大 的 变化 。 d . 经过 b c 后 ， 系统 终于 收敛 了 ， 但 带来 的 不良后果 就是 整个 收敛 速度 变慢 ， 因此 还 需要 增加 最大 迭代 次数 ， 经过 测试 迭代 次数 成 了 从 45w 修改 成 了 70w 。 e . 在 整个 运行 过程 中 ， 出现 了 几次 平稳 点 ， 20w 以及 40w 左右 的 时候 ， 因此 迭代 的 learning rate 应该 随着 迭代 的 接近 平稳 点 的 时候 有意 的 减小 一些 ， 目前 是 以 每 10w 次 减小 为 1 / 10 ， 调 参数 用 了 5 天 ， 最后 运行时间 为 15 天 。 f . 关于 调参 策略 ， 上面 只是 按照 一些 简单 的 理解 设置 的 ， 如果 没有 一个 合理 的 解释 ， 调参 就 变成 了 一个 很 low 的 工作 。 还好 发现 了 好几篇 关于 调参 的 论文 ， 主要 是 优化 算 法理 论 方面 的 ， 学习 完再 回来 测试 一下 。
